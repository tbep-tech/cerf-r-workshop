[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "About this Workshop\nThis workshop will provide attendees with a gentle introduction to the R Statistical Programming Language for basic data exploration and analysis. No prior experience is needed. The workshop content will include a mix of instructor led demonstrations and hands-on exercises designed to reinforce core principles of the programming language. Additional emphasis will be placed on using RStudio as the preferred platform for working with data in a project-based environment. By the end of the workshop, attendees will have the necessary foundation for continued learning to further build R skills.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Introduction to R",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\nTime\nTopic\n\n\n\n\n09:00\nR basics\n\n\n10:00\nBreak\n\n\n10:15\nData wrangling part 1\n\n\n11:15\nBreak\n\n\n11:30\nData wrangling part 2\n\n\n12:30\nLunch\n\n\n1:30\nData viz\n\n\n2:30\nBreak\n\n\n2:45\nSpatial data analysis\n\n\n3:45\nContinued learning and adjourn",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#housekeeping",
    "href": "index.html#housekeeping",
    "title": "Introduction to R",
    "section": "Housekeeping",
    "text": "Housekeeping\nThere are a few housekeeping items we need to go over before we start the workshop.\n\nFeel free to ask questions during the training. We will also use the stickies to indicate progress on exercises (green means okay, red means I need help).\nPlease use your own installation of RStudio or RStudio Cloud during this workshop. The setup instructions are here. You may use one or the other, but we recommend RStudio Cloud for the training.\nWe have a live coding link that we’ll be using as we go through the lessons. If you get lost, you can copy/paste code from this link into RStudio.\nAll training content is on this website. We will be covering the content directly on the website, so if you get lost you can view the agenda to see which lesson we’re covering and find where we’re at by scrolling through the content.\nEach lesson has it’s own R script that is linked at the top. If you are not using the live coding link, you can download the lesson R script and use that directly.\nFinally, if you are not using RStudio Cloud, please make sure to download the required datasets here and make sure you have the the tidyverse, sf, and mapview packages installed. See the setup instructions for details.\n\nAll exercises and breaks will be timed to make sure that we stay on schedule. We’ll display a timer on the screen to track progress.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "Introduction to R",
    "section": "Software requirements",
    "text": "Software requirements\nThis training will use RStudio and R. Please make sure that you are ready to use the required software prior to the workshop by following the setup instructions here. We will not be available for installation issues or questions the day of the workshop. There are two options:\n\nInstall RStudio and R on your personal computer (recommended).\nOR\nUse RStudio Cloud to access from a web browser.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#data-and-resources",
    "href": "index.html#data-and-resources",
    "title": "Introduction to R",
    "section": "Data and resources",
    "text": "Data and resources\nPlease view the Data and Resources page for data used in this workshop and additional links for R learning material. A live coding link is also available.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Introduction to R",
    "section": "Instructor",
    "text": "Instructor\nMarcus Beck (mbeck@tbep.org) - Marcus is the Senior Scientist for the Tampa Bay Estuary Program and is developing data analysis and visualization methods for Bay health indicators. He received his BS in Zoology from the University of Florida in 2007 and his MSc and PhD in Conservation Biology from the University of Minnesota in 2009 and 2013. Marcus has experience researching environmental indicators and developing open science products to support environmental decision-making. He has over fifteen years of experience using and developing software in the R Statistical Programming Language.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "index.html#source-content",
    "href": "index.html#source-content",
    "title": "Introduction to R",
    "section": "Source content",
    "text": "Source content\nAll source materials for this website can be accessed at https://github.com/tbep-tech/cerf-r-workshop\n This site is built automatically with GitHub Actions. Last site build 2025-10-21 13:43:15.512396.\n  This website is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "About this Workshop"
    ]
  },
  {
    "objectID": "R_basics.html",
    "href": "R_basics.html",
    "title": "1  R basics",
    "section": "",
    "text": "1.1 Lesson Outline\nGet the lesson R script: R_Basics.R\nGet the lesson data: download zip",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#lesson-outline",
    "href": "R_basics.html#lesson-outline",
    "title": "1  R basics",
    "section": "",
    "text": "Goals and Motivation\nRStudio\nR language fundamentals\nData structures in R\nGetting your data into R",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#lesson-exercises",
    "href": "R_basics.html#lesson-exercises",
    "title": "1  R basics",
    "section": "1.2 Lesson Exercises",
    "text": "1.2 Lesson Exercises\n\nExercise 1\nExercise 2\nExercise 3",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#goals-and-motivation",
    "href": "R_basics.html#goals-and-motivation",
    "title": "1  R basics",
    "section": "1.3 Goals and Motivation",
    "text": "1.3 Goals and Motivation\nR is a language for statistical computing as well as a general purpose programming language. It is one of the best languages used in data science and for data analysis.\nThe goals of this training are to expose you to fundamentals and to develop an appreciation of what’s possible with this software. We also provide resources that you can use for follow-up learning on your own. You should be able to answer these questions at the end of this session:\n\nWhat is R and why should I use it?\nWhy would I use RStudio and RStudio projects?\nHow can I write, save, and run scripts in RStudio?\nWhere can I go for help?\nWhat are the basic data structures in R?\nHow do I import data?\n\n\n1.3.1 Why should I invest time in R?\nThere are many programming languages available and each has it’s specific benefits. R was originally created as a statistical programming language but now it is largely viewed as a ‘data science’ language. Why would you invest time in learning R compared to other languages?\n\nThe growth of R as explained in the Stack Overflow blog, IEEE rating\n\nR is also an open-source programming language - not only is it free, but this means anybody can contribute to it’s development. As of 2025-10-21, there are 22919 supplemental packages for R on CRAN!",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#rstudio",
    "href": "R_basics.html#rstudio",
    "title": "1  R basics",
    "section": "1.4 RStudio",
    "text": "1.4 RStudio\nIn the old days, the only way to use R was directly from the Console - this is a bare bones way of running R only with direct input of commands. Now, RStudio is the go-to Interactive Development Environment (IDE) for R. Think of it like a car that is built around an engine. It is integrated with the console (engine) and includes many other features to improve the user’s experience, such as version control, debugging, dynamic documents, package manager and creation, and code highlighting and completion.\nLet’s get familiar with RStudio before we go on.\n\n1.4.1 Open R and RStudio\nIf you haven’t done so, download and install RStudio from the link above. After it’s installed, find the RStudio shortcut and fire it up (just watch for now). You should see something like this:\n\nThere are four panes in RStudio:\n\nSource: Your primary window for writing code to send to the console, this is where you write and save R “scripts”\nConsole: This is where code is executed in R - you should almost never write code here.\nEnvironment, History, etc.: A tabbed window showing your working environment, code execution history, and other useful things\nFiles, plots, etc.: A tabbed window showing a file explorer, a plot window, list of installed packages, help files, and viewer\n\n\n\n1.4.2 RStudio projects\nI strongly encourage you to use RStudio projects when you are working with R. The RStudio project provides a central location for working on a particular task. It helps with file management and is portable because all the files live in the same project. RStudio projects also remember history - what commands you used and what data objects are in your environment.\nTo create a new project, click on the File menu at the top and select ‘New project…’\n\nNow we can use this project for our data and any scripts we create.\n\n\n1.4.3 Scripting\nIn most cases, you will not enter and execute code directly in the console. Code can be written in a script and then sent directly to the console when you’re ready to run it. The key difference here is that a script can be saved and shared.\nOpen a new script from the File menu…\n\n\n\n1.4.4 Executing code in RStudio\nAfter you write code in your script, it can be sent to the Console to run the code in R. Anything you write in the script will not be run or saved in R until it is sent to the console. There are two ways to do this. First, you can hit the Run button at the top right of the scripting window. Second, you can use ctrl+enter (cmd+enter on a Mac). Both approaches will send the selected line to the console, then move to the next line in your script. You can also highlight and send an entire block of code.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#exercise-1",
    "href": "R_basics.html#exercise-1",
    "title": "1  R basics",
    "section": "1.5 Exercise 1",
    "text": "1.5 Exercise 1\nThis exercise will make sure R and RStudio are working and that you can get around the basics in RStudio.\n\nStart RStudio if using a version you installed OR navigate to https://posit.cloud/project/11213234 if using RStudio Cloud. To start both R and RStudio requires only firing up RStudio. RStudio should be available from All Programs at the Start Menu. Fire up RStudio.\nIf you’re not using RStudio Cloud, create a new project (File menu, New project, New directory, New project, Directory Name…). Name it “r_workshop”. We will use this for the rest of the workshop.\nCreate a new “R Script” in the Source Pane, save that file into your newly created project and name it “first_script.R”. It’ll just be a blank text file at this point.\nAdd in a comment line to separate this section. It should look something like: # Exercise 1: Just Getting used to RStudio and Scripts.\nLastly, we need to get this project set up with some example data for our exercises (if you’re using RStudio Cloud, ignore this step). You should have downloaded this already, but if not, the data are available here. The data are in a zipped folder. Download the file to your computer (anywhere). Create a folder in your new project named data and extract the files into this location.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#r-language-fundamentals",
    "href": "R_basics.html#r-language-fundamentals",
    "title": "1  R basics",
    "section": "1.6 R language fundamentals",
    "text": "1.6 R language fundamentals\nR is built around functions. These are commands that do specific things based on what you provide. The basic syntax of a function follows the form: function_name(arg1, arg2, ...).\nWith the base install, you will gain access to many functions (2358, to be exact). Some examples:\n\n# print\nprint('hello world!')\n\n[1] \"hello world!\"\n\n# sequence\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# random numbers\nrnorm(100, mean = 10, sd = 2)\n\n  [1] 12.079010  8.586937 14.154825 13.936399  6.883798  6.305395  9.084455\n  [8] 12.829594 10.887918  7.836530  9.001568 10.370287  9.831415 10.245919\n [15] 14.092140  8.937508 11.047354 14.423350 12.034981 11.138949  7.477300\n [22]  9.908831 11.669912 11.020338 12.686709  9.032382 14.305234  8.766064\n [29]  9.723696 10.154350  8.939600 11.200230 10.506619  9.078964 10.986311\n [36] 11.654388  9.244846  7.355144 12.126700 13.771073 10.276413 12.012653\n [43] 11.421143 10.202323 11.937957  8.573577  9.025359  8.229556 11.470701\n [50]  9.097463 10.058142  9.600978  9.457922  9.417648  8.919053  4.272084\n [57] 12.720254 12.421122 12.140990  8.649492  9.583118  8.582591 10.244622\n [64] 14.277907  6.458151  8.662173  9.287872  8.062564  9.676604  7.489945\n [71] 13.429654  7.299800  5.678875  9.012768  9.826495  8.149252 11.165402\n [78] 11.112514  9.908719 11.215820 11.194903 10.292050 10.905226 10.633970\n [85] 12.108688  7.833701  8.028722 12.372297  9.763606  7.939484 12.896091\n [92]  6.120249 11.898701 12.703183  9.619425  9.661342 12.074042 10.005690\n [99] 11.061344 10.353844\n\n# average \nmean(rnorm(100))\n\n[1] 0.07598035\n\n# sum\nsum(rnorm(100))\n\n[1] 12.25621\n\n\nVery often you will see functions used like this:\n\nmy_random_sum &lt;- sum(rnorm(100))\n\nIn this case the first part of the line is the name of an object. You make this up. Ideally it should have some meaning, but the only rules are that it can’t start with a number and must not have any spaces. The second bit, &lt;-, is the assignment operator. This tells R to take the result of sum(rnorm(100)) and store it in an object named, my_random_sum. It is stored in the environment and can be used by just executing it’s name in the console.\n\nmy_random_sum\n\n[1] 10.75606\n\n\n\n1.6.1 What is the environment?\nThere are two outcomes when you run code. First, the code will simply print output directly in the console. Second, there is no output because you have stored it as a variable using &lt;-. Output that is stored is actually saved in the environment. The environment is the collection of named objects that are stored in memory for your current R session. Anything stored in memory will be accessible by it’s name without running the original script that was used to create it.\nWith this, you have the very basics of how we write R code and save objects that can be used later.\n\n\n1.6.2 Packages\nThe base install of R is quite powerful, but you will soon have a need or desire to go beyond this. Packages provide this ability. They are a standardized way of extending R with new methods, techniques, and programming functionality. There is a lot to say about packages regarding finding them, using them, etc., but for now let’s focus just on the basics.\n\n\n1.6.3 CRAN\nOne of the reasons for R’s popularity is CRAN, The Comprehensive R Archive Network. This is where you download R and also where most will gain access to packages (there are other places, but that is for later). Not much else to say about this now other than to be aware of it. As of 2025-10-21, there are 22919 packages on CRAN!\n\n\n1.6.4 Installing packages\nWhen a package gets installed, that means the source code is downloaded and put into your library. A default library location is set for you so no need to worry about that. In fact, on Windows most of this is pretty automatic. Let’s give it a shot.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#exercise-2",
    "href": "R_basics.html#exercise-2",
    "title": "1  R basics",
    "section": "1.7 Exercise 2",
    "text": "1.7 Exercise 2\nWe’re going to install some packages from CRAN that will give us the tools for our workshop today. We’ll use the tidyverse, sf, and mapview packages. Later, we’ll explain in detail what each of these packages provide. Again, if you are using RStudio Cloud, these packages will already be installed. You can skip to step 5 in this case.\n\nAt the top of the script you just created, type the following functions.\n\n# install packages from CRAN\ninstall.packages(\"tidyverse\")\ninstall.packages(\"sf\")\ninstall.packages(\"mapview\")\n\nSelect all the lines by clicking and dragging the mouse pointer over the text.\nSend all the commands to the console using ctrl+enter. You should see some text output on the console about the installation process. The installation may take a few minutes so don’t be alarmed.\nAfter the packages are done installing, verify that there were no errors during the process (this should be pretty obvious, i.e., error text in big scary red letters).\nLoad the packages after they’ve installed.\n\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"mapview\")\n\n\nAn important aspect of packages is that you only need to download them once, but every time you start RStudio you need to load them with the library() function. Loading a package makes all of its functions available in your current R session.\n\n1.7.1 Getting Help\nBeing able to find help and interpret that help is probably one of the most important skills for learning a new language. R is no different. Help on functions and packages can be accessed directly from R, can be found on CRAN and other official R resources, searched on Google, found on StackOverflow, or from any number of fantastic online resources. I will cover a few of these here.\n\n\n1.7.2 Help from the console\nGetting help from the console is straightforward and can be done numerous ways.\n\n# Using the help command/shortcut\n# When you know the name of a function\nhelp(\"print\") # Help on the print command\n?print # Help on the print command using the `?` shortcut\n\n# When you know the name of the package\nhelp(package = \"sf\") # Help on the package `dplyr`\n\n# Don't know the exact name or just part of it\napropos(\"print\") # Returns all available functions with \"print\" in the name\n??print # shortcut, but also searches demos and vignettes in a formatted page\n\n\n\n1.7.3 Official R Resources\nIn addition to help from within R itself, CRAN and the R-Project have many resources available for support. Two of the most notable are the mailing lists and the task views.\n\nR Help Mailing List: The main mailing list for R help. Can be a bit daunting and some (although not most) senior folks can be, um, curmudgeonly…\nR-sig-ecology: A special interest group for use of R in ecology. Less daunting than the main help with participation from some big names in ecological modelling and statistics (e.g., Ben Bolker, Gavin Simpson, and Phil Dixon).\nEnvironmetrics Task View: Task views are great in that they provide an annotated list of packages relevant to a particular field. This one is maintained by Gavin Simpson and has great info on packages relevant to the environmental sciences.\nSpatial Analysis Task View: One I use a lot that lists all the relevant packages for spatial analysis, GIS, and Remote Sensing in R.\n\n\n\n1.7.4 Google and StackOverflow\nWhile the resources already mentioned are useful, often the quickest way is to just turn to Google. However, a search for “R” is a bit challenging. A few ways around this. Google works great if you search for a given package or function name. You can also search for mailing lists directly (i.e. “R-sig-geo”), although Google often finds results from these sources.\nBlind googling can require a bit of strategy to get the info you want. Some pointers:\n\nAlways preface the search with “r”\nUnderstand which sources are reliable\nTake note of the number of hits and date of a web page\nWhen in doubt, search with the exact error message (see here for details about warnings vs errors)\n\nOne specific resource that I use quite a bit is StackOverflow with the ‘r’ tag. StackOverflow is a discussion forum for all things related to programming. You can then use this tag and the search functions in StackOverflow and find answers to almost anything you can think of. However, these forums are also very strict and I typically use them to find answers, not to ask questions.\n\n\n1.7.5 Other Resources\nAs I mentioned earlier, there are TOO many resources to list here and everyone has their favorites. Below are just a few that I like.\n\nR For Cats: Basic introduction site, meant to be a gentle and light-hearted introduction\nR for Data Science: My favorite resource for learning data science methods with R, focusing on the tidyverse. Much of the content in this training is adapted from this book.\nAdvanced R: Web home of Hadley Wickham’s book, the same author of R for Data Science. Gets into more advanced topics, but also covers the basics in a great way.\nCRAN Cheatsheets: A good cheat sheet from the official source\nRStudio Cheatsheets: Additional cheat sheets from RStudio. I am especially fond of the data wrangling one.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#data-structures-in-r",
    "href": "R_basics.html#data-structures-in-r",
    "title": "1  R basics",
    "section": "1.8 Data structures in R",
    "text": "1.8 Data structures in R\nNow that you know how to get started in R and where to find resources, we can begin talking about R data structures. Simply put, a data structure is a way for programming languages to handle information storage.\nThere is a bewildering amount of formats for storing data and R is no exception. Understanding the basic building blocks that make up data types is essential. All functions in R require specific types of input data and the key to using functions is knowing how these types relate to each other.\n\n1.8.1 Vectors (one-dimensional data)\nThe basic data format in R is a vector - a one-dimensional grouping of elements that have the same type. These are all vectors and they are created with the c function:\n\ndbl_var &lt;- c(1, 2.5, 4.5)\nint_var &lt;- c(1L, 6L, 10L)\nlog_var &lt;- c(TRUE, FALSE, T, F)\nchr_var &lt;- c(\"a\", \"b\", \"c\")\n\nThe four types of atomic vectors (think atoms that make up a molecule aka vector) are double (or numeric), integer, logical, and character. For most purposes you can ignore the integer class, so there are basically three types. Each type has some useful properties:\n\nclass(dbl_var)\n\n[1] \"numeric\"\n\nlength(log_var)\n\n[1] 4\n\n\nThese properties are useful for not only describing an object, but they define limits on which functions or types of operations that can be used. That is, some functions require a character string input while others require a numeric input. Similarly, vectors of different types or properties may not play well together. Let’s look at some examples:\n\n# taking the mean of a character vector\nmean(chr_var)\n\n# adding two numeric vectors of different lengths\nvec1 &lt;- c(1, 2, 3, 4)\nvec2 &lt;- c(2, 3, 5)\nvec1 + vec2\n\n\n\n1.8.2 2-dimensional data\nA collection of vectors represented as one data object are often described as two-dimensional data, or in R speak, a data frame (i.e., data.frame()). Think of them like your standard spreadsheet, where each column describes a variable (vector) and rows link observations between columns. Here’s a simple example:\n\nltrs &lt;- c('a', 'b', 'c')\nnums &lt;- c(1, 2, 3)\nlogs &lt;- c(T, F, T)\nmydf &lt;- data.frame(ltrs, nums, logs)\nmydf\n\n  ltrs nums  logs\n1    a    1  TRUE\n2    b    2 FALSE\n3    c    3  TRUE\n\n\nThe only constraints required to make a data frame are:\n\nEach column (vector) contains the same type of data\nThe number of observations in each column is equal.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#getting-your-data-into-r",
    "href": "R_basics.html#getting-your-data-into-r",
    "title": "1  R basics",
    "section": "1.9 Getting your data into R",
    "text": "1.9 Getting your data into R\nIt is the rare case when you manually enter your data in R, not to mention impractical for most datasets. Most data analysis workflows typically begin with importing a dataset from an external source. Literally, this means committing a dataset to memory (i.e., storing it as a variable) as one of R’s data structure formats.\nFlat data files (text only, rectangular format) present the least complications on import because there is very little to assume about the structure of the data. On import, R tries to guess the data type for each column and this is fairly unambiguous with flat files. We’ll be using read_csv() function from the readr package that comes with the tidyverse.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#the-working-directory",
    "href": "R_basics.html#the-working-directory",
    "title": "1  R basics",
    "section": "1.10 The working directory",
    "text": "1.10 The working directory\nBefore we import data, we need to talk about the “working directory”. Whenever RStudio is opened, it uses a file location on your computer to access and save data. If you’re using an RStudio project, the working directory will be the folder where you created the project. If not, it is probably the the user’s home directory (e.g., C:/Users/Marcus), which you’ll want to change to where you have your data.\nYou can see your working directory with the getwd() function or from the file path at the top of the console in RStudio. All files in the File pane window on the bottom right of RStudio are also those within the working directory. If you want to change your working directory, you can use the setwd() function and put the file path (as a character string) inside the function, e.g., setwd('C:/Users/Marcus/Desktop/newdirectory').\nThe working directory is important to know when you’re importing or exporting data. When you import data, a relative file path can be used that is an extension of the working directory. For example, if your working directory is 'C:/Users/Marcus/Desktop' and you have a file called mydata.csv in that directory, you can use read_csv('mydata.csv') to import the file. Alternatively, if there’s a folder called “data” in your working directory with the file you want to import, you would use read_csv('data/mydata.csv').\nIf you want to import a file that is not in your working directory, you will have to use an absolute path that is the full file location. Otherwise, R will not know where to look outside of the working directory.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#exercise-3",
    "href": "R_basics.html#exercise-3",
    "title": "1  R basics",
    "section": "1.11 Exercise 3",
    "text": "1.11 Exercise 3\nNow that we have the data downloaded and extracted to our data folder, we’ll use read_csv to import two files into our environment. The read_csv function comes with the tidyverse package, so make sure that package is loaded (i.e., library(tidyverse)) before you do this exercise. This should have been done in the second exercise.\n\nType the following in your script. Note the use of relative file paths within your project (see the explanation above).\n\nfishdat &lt;- read_csv('data/fishdat.csv')\nstatloc &lt;- read_csv('data/statloc.csv')\n\nSend the commands to the console with ctrl+enter.\nVerify that the data imported correctly by viewing the first six rows of each dataset. Use the head() function directly in the console, e.g., head(fishdat) or head(statloc)\n\nLet’s explore the datasets a bit. There are many useful functions for exploring the characteristics of a dataset. This is always a good idea when you first import something.\n\n# get the dimensions\ndim(fishdat)\n\n[1] 2844   12\n\ndim(statloc)\n\n[1] 2173    3\n\n# get the column names\nnames(fishdat)\n\n [1] \"OBJECTID\"      \"Reference\"     \"Sampling_Date\" \"yr\"           \n [5] \"Gear\"          \"ExDate\"        \"Bluefish\"      \"Common Snook\" \n [9] \"Mullets\"       \"Pinfish\"       \"Red Drum\"      \"Sand Seatrout\"\n\nnames(statloc)\n\n[1] \"Reference\" \"Latitude\"  \"Longitude\"\n\n# see the first six rows\nhead(fishdat)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\nhead(statloc)\n\n# A tibble: 6 × 3\n  Reference     Latitude Longitude\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 TBM1996032006     27.9     -82.6\n2 TBM1996032004     27.9     -82.6\n3 TBM1996032207     27.9     -82.5\n4 TBM1996042601     28.0     -82.7\n5 TBM1996051312     27.9     -82.6\n6 TBM1996051407     27.9     -82.6\n\n# get the overall structure\nstr(fishdat)\n\nspc_tbl_ [2,844 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ OBJECTID     : num [1:2844] 1550020 1550749 1550750 1550762 1550828 ...\n $ Reference    : chr [1:2844] \"TBM1996032006\" \"TBM1996032004\" \"TBM1996032004\" \"TBM1996032207\" ...\n $ Sampling_Date: Date[1:2844], format: \"1996-03-20\" \"1996-03-20\" ...\n $ yr           : num [1:2844] 1996 1996 1996 1996 1996 ...\n $ Gear         : num [1:2844] 300 22 22 20 160 300 300 300 300 22 ...\n $ ExDate       : POSIXct[1:2844], format: \"2018-04-12 10:27:38\" \"2018-04-12 10:25:23\" ...\n $ Bluefish     : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Common Snook : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Mullets      : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Pinfish      : num [1:2844] 0 54 0 80 0 0 0 0 1 1 ...\n $ Red Drum     : num [1:2844] 0 0 1 0 4 0 0 0 0 0 ...\n $ Sand Seatrout: num [1:2844] 1 0 0 0 0 1 5 66 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   OBJECTID = col_double(),\n  ..   Reference = col_character(),\n  ..   Sampling_Date = col_date(format = \"\"),\n  ..   yr = col_double(),\n  ..   Gear = col_double(),\n  ..   ExDate = col_datetime(format = \"\"),\n  ..   Bluefish = col_double(),\n  ..   `Common Snook` = col_double(),\n  ..   Mullets = col_double(),\n  ..   Pinfish = col_double(),\n  ..   `Red Drum` = col_double(),\n  ..   `Sand Seatrout` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(statloc)\n\nspc_tbl_ [2,173 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Reference: chr [1:2173] \"TBM1996032006\" \"TBM1996032004\" \"TBM1996032207\" \"TBM1996042601\" ...\n $ Latitude : num [1:2173] 27.9 27.9 27.9 28 27.9 ...\n $ Longitude: num [1:2173] -82.6 -82.6 -82.5 -82.7 -82.6 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Reference = col_character(),\n  ..   Latitude = col_double(),\n  ..   Longitude = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nYou can also view each dataset in a spreadsheet style in the scripting window:\n\nView(fishdat)\nView(statloc)\n\n\n1.11.1 Other ways to import data\nYou might want to import an Excel spreadsheet as well. In the old days, importing spreadsheets into R was almost impossible given the proprietary data structure used by Microsoft. The tools available in R have since matured and it’s now pretty painless to import a spreadsheet. The readxl package is the most recent and by far most flexible data import package for Excel files. It comes with the tidyverse family of packages.\nOnce installed, we can load it to access the import functions.\n\nlibrary(readxl)\ndat &lt;- read_excel('location/of/excel/file.xlsx')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "R_basics.html#summary",
    "href": "R_basics.html#summary",
    "title": "1  R basics",
    "section": "1.12 Summary",
    "text": "1.12 Summary\nIn this lesson we learned about R and Rstudio, some of the basic syntax and data structures in R, and how to import files. We’ve just imported some provisional fisheries data from the FWRI FIM database for Old Tampa Bay (OTB) that we’ll continue to use for the rest of the workshop. Next we’ll learn how to process and plot these data to gain insight into how these data vary through space and time.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R basics</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html",
    "href": "data_wrangling_1.html",
    "title": "2  Data wrangling part 1",
    "section": "",
    "text": "2.1 Lesson Outline\nGet the lesson R script: Data_Wrangling_1.R\nGet the lesson data: download zip",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#lesson-outline",
    "href": "data_wrangling_1.html#lesson-outline",
    "title": "2  Data wrangling part 1",
    "section": "",
    "text": "Goals\nThe tidyverse\nData wrangling with dplyr\nPiping",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#lesson-exercises",
    "href": "data_wrangling_1.html#lesson-exercises",
    "title": "2  Data wrangling part 1",
    "section": "2.2 Lesson Exercises",
    "text": "2.2 Lesson Exercises\n\nExercise 4\nExercise 5",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#goals",
    "href": "data_wrangling_1.html#goals",
    "title": "2  Data wrangling part 1",
    "section": "2.3 Goals",
    "text": "2.3 Goals\nData wrangling (manipulation, ninjery, cleaning, etc.) is the part of any data analysis that will take the most time. While it may not necessarily be fun, it is foundational to all the work that follows. I strongly believe that mastering these skills has more value than mastering a particular analysis. Check out this article if you don’t believe me.\nWe’ll have two hours to cover parts 1 and 2 of data wrangling. It’s an unrealistic expectation that you will be a ninja wrangler after this training. As such, the goals are to expose you to fundamentals and to develop an appreciation of what’s possible. I also want to provide resources that you can use for follow-up learning on your own.\nAfter this lesson you should be able to answer (or be able to find answers to) the following:\n\nWhy do we need to manipulate data?\nWhat is the tidyverse?\nWhat can you do with dplyr?\nWhat is piping?",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#the-tidyverse",
    "href": "data_wrangling_1.html#the-tidyverse",
    "title": "2  Data wrangling part 1",
    "section": "2.4 The tidyverse",
    "text": "2.4 The tidyverse\nThe tidyverse is a set of packages that work in harmony because they share common data representations and design. The tidyverse package is designed to make it easy to install and load core packages from the tidyverse in a single command. With tidyverse, you’ll be able to address all steps of data exploration.\n\nFrom the excellent book, R for Data Science, data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. Tools in the tidyverse also have direct application to more formal analyses with many of the other available R packages on CRAN.\nYou should already have the tidyverse installed, but let’s give it a go if you haven’t done this part yet:\n\n# install\ninstall.packages('tidyverse')\n\nAfter installation, we can load the package:\n\n# load\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nNotice that the messages you get after loading are a bit different from other packages. That’s because tidyverse is a package that manages other packages. Loading tidyverse will load all of the core packagbes:\n\nggplot2, for data visualisation.\ndplyr, for data manipulation.\ntidyr, for data tidying.\nreadr, for data import.\npurrr, for functional programming.\ntibble, for tibbles, a modern re-imagining of data frames.\n\nOther packages (e.g., readxl) are also included but you will probably not use these as frequently.\nA nice feature of tidyverse is the ability to check for and install new versions of each package:\ntidyverse_update()\n#&gt; The following packages are out of date:\n#&gt;  * broom (0.4.0 -&gt; 0.4.1)\n#&gt;  * DBI   (0.4.1 -&gt; 0.5)\n#&gt;  * Rcpp  (0.12.6 -&gt; 0.12.7)\n#&gt; Update now?\n#&gt;\n#&gt; 1: Yes\n#&gt; 2: No\nAs you’ll soon learn using R, there are often several ways to achieve the same goal. The tidyverse provides tools to address problems that can be solved with other packages or even functions from the base installation. Tidyverse is admittedly an opinionated approach to data exploration, but it’s popularity and rapid growth within the R community is a testament to the power of the tools that are provided.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#data-wrangling-with-dplyr",
    "href": "data_wrangling_1.html#data-wrangling-with-dplyr",
    "title": "2  Data wrangling part 1",
    "section": "2.5 Data wrangling with dplyr",
    "text": "2.5 Data wrangling with dplyr\n\nThe data wrangling process includes data import, tidying, and transformation. The process directly feeds into, and is not mutually exclusive, with the understanding or modelling side of data exploration. More generally, I consider data wrangling as the manipulation or combination of datasets for the purpose of analysis.\nWrangling begins with import and ends with an output of some kind, such as a plot or a model, but is more often a dataset that has been altered from a raw dataset to suit the needs of an analysis. In a perfect world, the wrangling process is linear with no need for back-tracking. In reality, we often uncover more information about a dataset, either through wrangling or modeling, that warrants re-evaluation or even gathering more data. Data also come in many forms and the form you need for analysis is rarely the required form of the input data. For these reasons, data wrangling will consume most of your time in data exploration.\nAll wrangling is based on a purpose. No one wrangles for the sake of wrangling (usually), so the process always begins by answering the following two questions:\n\nWhat do my input data look like?\nWhat should my input data look like given what I want to do?\n\nAt the most basic level, going from what your data looks like to what it should look like will require a few key operations. Some common examples:\n\nSelecting specific variables\nFiltering observations by some criteria\nAdding or modifying existing variables\nRenaming variables\nArranging rows by a variable\nSummarizing variable conditional on others\n\nThe dplyr package provides easy tools for these common data manipulation tasks. It is built to work directly with data frames and this is one of the foundational packages in what is now known as the tidyverse. The philosophy of dplyr is that one function does one thing and the name of the function says what it does. This is where the tidyverse generally departs from other packages and even base R. It is meant to be easy, logical, and intuitive. There is a lot of great info on dplyr. If you have an interest, I’d encourage you to look more. The vignettes are particularly good.\n\ndplyr GitHub repo\nCRAN page: vignettes here\nCheatsheet\n\nI’ll demonstrate the examples with the fishdat dataset from the last lesson. This dataset includes over 30000 catch records of fish from Old Tampa Bay.\n\n# import the data\nfishdat &lt;- read_csv('data/fishdat.csv')\n\n# see first six rows\nhead(fishdat)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# dimensions\ndim(fishdat)\n\n[1] 2844   12\n\n# column names\nnames(fishdat)\n\n [1] \"OBJECTID\"      \"Reference\"     \"Sampling_Date\" \"yr\"           \n [5] \"Gear\"          \"ExDate\"        \"Bluefish\"      \"Common Snook\" \n [9] \"Mullets\"       \"Pinfish\"       \"Red Drum\"      \"Sand Seatrout\"\n\n# structure\nstr(fishdat)\n\nspc_tbl_ [2,844 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ OBJECTID     : num [1:2844] 1550020 1550749 1550750 1550762 1550828 ...\n $ Reference    : chr [1:2844] \"TBM1996032006\" \"TBM1996032004\" \"TBM1996032004\" \"TBM1996032207\" ...\n $ Sampling_Date: Date[1:2844], format: \"1996-03-20\" \"1996-03-20\" ...\n $ yr           : num [1:2844] 1996 1996 1996 1996 1996 ...\n $ Gear         : num [1:2844] 300 22 22 20 160 300 300 300 300 22 ...\n $ ExDate       : POSIXct[1:2844], format: \"2018-04-12 10:27:38\" \"2018-04-12 10:25:23\" ...\n $ Bluefish     : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Common Snook : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Mullets      : num [1:2844] 0 0 0 0 0 0 0 0 0 0 ...\n $ Pinfish      : num [1:2844] 0 54 0 80 0 0 0 0 1 1 ...\n $ Red Drum     : num [1:2844] 0 0 1 0 4 0 0 0 0 0 ...\n $ Sand Seatrout: num [1:2844] 1 0 0 0 0 1 5 66 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   OBJECTID = col_double(),\n  ..   Reference = col_character(),\n  ..   Sampling_Date = col_date(format = \"\"),\n  ..   yr = col_double(),\n  ..   Gear = col_double(),\n  ..   ExDate = col_datetime(format = \"\"),\n  ..   Bluefish = col_double(),\n  ..   `Common Snook` = col_double(),\n  ..   Mullets = col_double(),\n  ..   Pinfish = col_double(),\n  ..   `Red Drum` = col_double(),\n  ..   `Sand Seatrout` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n2.5.1 Selecting\nLet’s begin using dplyr. Don’t forget to load the tidyverse if you haven’t done so already. We can use the select function to, you guessed it, select columns.\n\n# first, select some columns\ndplyr_sel1 &lt;- select(fishdat, Sampling_Date, Gear, Pinfish)\nhead(dplyr_sel1)\n\n# A tibble: 6 × 3\n  Sampling_Date  Gear Pinfish\n  &lt;date&gt;        &lt;dbl&gt;   &lt;dbl&gt;\n1 1996-03-20      300       0\n2 1996-03-20       22      54\n3 1996-03-20       22       0\n4 1996-03-22       20      80\n5 1996-04-26      160       0\n6 1996-05-13      300       0\n\n# select everything but ObjectId and ExDate\ndplyr_sel2 &lt;- select(fishdat, -OBJECTID, -ExDate)\nhead(dplyr_sel2)\n\n# A tibble: 6 × 10\n  Reference    Sampling_Date    yr  Gear Bluefish `Common Snook` Mullets Pinfish\n  &lt;chr&gt;        &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 TBM19960320… 1996-03-20     1996   300        0              0       0       0\n2 TBM19960320… 1996-03-20     1996    22        0              0       0      54\n3 TBM19960320… 1996-03-20     1996    22        0              0       0       0\n4 TBM19960322… 1996-03-22     1996    20        0              0       0      80\n5 TBM19960426… 1996-04-26     1996   160        0              0       0       0\n6 TBM19960513… 1996-05-13     1996   300        0              0       0       0\n# ℹ 2 more variables: `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# select columns that contain the letter c\ndplyr_sel3 &lt;- select(fishdat, matches('c'))\nhead(dplyr_sel3)\n\n# A tibble: 6 × 3\n  OBJECTID Reference     `Common Snook`\n     &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1  1550020 TBM1996032006              0\n2  1550749 TBM1996032004              0\n3  1550750 TBM1996032004              0\n4  1550762 TBM1996032207              0\n5  1550828 TBM1996042601              0\n6  1550838 TBM1996051312              0\n\n\n\n\n2.5.2 Filtering\nAfter selecting columns, you’ll probably want to remove observations that don’t fit some criteria. For example, maybe you want to remove all rows with low catch or maybe you want to look only at fish caught in Gear 20 (a 21 m seine net).\n\n# now filter observations with more than 30 Pinfish caught\ndplyr_high_catch &lt;- filter(fishdat, Pinfish &gt; 30)\nhead(dplyr_high_catch)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n2  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n3  1555103 TBM1996022010 1996-02-20     1996    22 2018-04-12 10:25:29        0\n4  1572931 TBM1996080603 1996-08-06     1996   160 2018-04-12 10:25:36        0\n5  1572932 TBM1996080604 1996-08-06     1996   160 2018-04-12 10:25:36        0\n6  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# now filter observations for Gear type as 20\ndplyr_gear20 &lt;- filter(fishdat, Gear == 20)\nhead(dplyr_gear20)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n2  1555259 TBM1996022104 1996-02-21     1996    20 2018-04-12 10:25:31        0\n3  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n4  1576651 TBM1996022007 1996-02-20     1996    20 2018-04-12 10:25:24        0\n5  1584465 TBM1996061917 1996-06-19     1996    20 2018-04-12 10:26:04        0\n6  1584803 TBM1996011905 1996-01-19     1996    20 2018-04-12 10:25:26        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n\nFiltering can take a bit of time to master because there are several ways to tell R what you want. Within the filter function, the working part is a logical selection of TRUE and FALSE values that are used to select rows (TRUE means I want that row, FALSE means I don’t). Every selection within the filter function, no matter how complicated, will always be a T/F vector. This is similar to running queries on a database if you’re familiar with SQL.\nTo use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). When you’re starting out with R, the easiest mistake to make is to use = instead of == when testing for equality.\nMultiple arguments to filter() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: & is “and”, | is “or”, and ! is “not”. This is the complete set of Boolean operations.\n\nLet’s start combining filtering operations.\n\n# get rows with &gt; 30 and less than 100 Pinfish\nfilt1 &lt;- filter(fishdat, Pinfish &gt; 30 & Pinfish &lt; 100)\nhead(filt1)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n2  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n3  1555103 TBM1996022010 1996-02-20     1996    22 2018-04-12 10:25:29        0\n4  1572931 TBM1996080603 1996-08-06     1996   160 2018-04-12 10:25:36        0\n5  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n6  1635253 TBM1997031016 1997-03-10     1997    20 2018-04-12 10:29:33        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# get rows with gear type 20 or red drum larger than 40\nfilt2 &lt;- filter(fishdat, Gear == 20 | `Red Drum` &gt; 40)\nhead(filt2)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n2  1555259 TBM1996022104 1996-02-21     1996    20 2018-04-12 10:25:31        0\n3  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n4  1576651 TBM1996022007 1996-02-20     1996    20 2018-04-12 10:25:24        0\n5  1584465 TBM1996061917 1996-06-19     1996    20 2018-04-12 10:26:04        0\n6  1584803 TBM1996011905 1996-01-19     1996    20 2018-04-12 10:25:26        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# get rows gear 20 or gear 5\nfilt3 &lt;- filter(fishdat, Gear == 20 | Gear == 5)\nhead(filt3)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n2  1555259 TBM1996022104 1996-02-21     1996    20 2018-04-12 10:25:31        0\n3  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n4  1576651 TBM1996022007 1996-02-20     1996    20 2018-04-12 10:25:24        0\n5  1584465 TBM1996061917 1996-06-19     1996    20 2018-04-12 10:26:04        0\n6  1584803 TBM1996011905 1996-01-19     1996    20 2018-04-12 10:25:26        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# get rows with gear 20 or gear 5 using different syntax\nfilt4 &lt;- filter(fishdat, Gear %in% c(20, 5))\nhead(filt4)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n2  1555259 TBM1996022104 1996-02-21     1996    20 2018-04-12 10:25:31        0\n3  1573079 TBM1996062104 1996-06-21     1996    20 2018-04-12 10:26:15        0\n4  1576651 TBM1996022007 1996-02-20     1996    20 2018-04-12 10:25:24        0\n5  1584465 TBM1996061917 1996-06-19     1996    20 2018-04-12 10:26:04        0\n6  1584803 TBM1996011905 1996-01-19     1996    20 2018-04-12 10:25:26        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n\nAs a side note, any variable (i.e., column name) with spaces or non-standard characters can be referenced by enclosing it with backticks (see here).\n\n\n2.5.3 Mutating\nNow that we’ve seen how to filter observations and select columns of a data frame, maybe we want to add a new column. In dplyr, mutate allows us to add new columns. These can be vectors you are adding or based on expressions applied to existing columns. For instance, we have a column for average size in mm and maybe we want to convert to cm.\n\n# add a column as bluefish divided by 100\ndplyr_mut &lt;- mutate(fishdat, Bluefish_p100 = Bluefish / 100)\nhead(dplyr_mut)\n\n# A tibble: 6 × 13\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 6 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;, Bluefish_p100 &lt;dbl&gt;\n\n# add a column for many/few mullet\ndplyr_mut2 &lt;- mutate(fishdat, mullet_cat = ifelse(Mullets &lt; 20, 'few', 'many'))\nhead(dplyr_mut2)\n\n# A tibble: 6 × 13\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 6 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;, mullet_cat &lt;chr&gt;\n\n\nSome other useful dplyr functions include arrange to sort the observations (rows) by a column and rename to (you guessed it) rename a column.\n\n# arrange by maximum size\ndplyr_arr &lt;- arrange(fishdat, `Sand Seatrout`)\nhead(dplyr_arr)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n2  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n4  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n5  1551311 TBM1996032209 1996-03-22     1996   300 2018-04-12 10:25:20        0\n6  1551335 TBM1996041807 1996-04-18     1996    22 2018-04-12 10:25:24        0\n# ℹ 5 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n# rename some columns\ndplyr_rnm &lt;- rename(fishdat, snook = `Common Snook`)\nhead(dplyr_rnm)\n\n# A tibble: 6 × 12\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 5 more variables: snook &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;\n\n\nThere are many more functions in dplyr but the ones above are by far the most used. As you can imagine, they are most effective when used together because there is never only one step in the data wrangling process. After the exercise, we’ll talk about how we can efficiently pipe the functions in dplyr to create a new data object.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#exercise-4",
    "href": "data_wrangling_1.html#exercise-4",
    "title": "2  Data wrangling part 1",
    "section": "2.6 Exercise 4",
    "text": "2.6 Exercise 4\nNow that you know the basic functions in dplyr and how to use them, let’s put them to use. Using fishdat, let’s select some columns of interest, filter by gear type, and rename one of the columns.\n\nSelect the Reference, Sampling_Date, Gear, and Sand Seatrout columns. Assign this dataset to a new object in your workspace. Don’t forget to use backticks to select column names that have spaces.\nUsing the new object, filter to get all rows where Gear is equal to 20 (hint, filter by the Gear column and don’t forget ot use ==).\nRename the Sampling_Date column to date.\n\n\n\nClick to show/hide solution\nex1 &lt;- select(fishdat, Reference, Sampling_Date, Gear, `Sand Seatrout`)\nex1 &lt;- filter(ex1, Gear == 20)\nex1 &lt;- rename(ex1, date = Sampling_Date)\nnrow(ex1)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#piping",
    "href": "data_wrangling_1.html#piping",
    "title": "2  Data wrangling part 1",
    "section": "2.7 Piping",
    "text": "2.7 Piping\nA complete data wrangling exercise will always include multiple steps to go from the raw data to the output you need. Here’s a terrible wrangling example using functions from base R:\n\ncropdat &lt;- rawdat[1:28]\nsavecols &lt;- data.frame(cropdat$Party, cropdat$`Last Inventory Year (2015)`)\nnames(savecols) &lt;- c('Party','2015')\nsavecols$rank2015 &lt;- rank(-savecols$`2015`)\ntop10df &lt;- savecols[savecols$rank2015 &lt;= 10,]\nbasedat &lt;- cropdat[cropdat$Party %in% top10df$Party,]\n\nTechnically, if this works it’s not “wrong”, but there are a couple of issues that can lead to problems down the line. First, the flow of functions to manipulate the data is not obvious and this makes your code very hard to read. Second, lots of unecessary intermediates have been created in your workspace. Anything that adds to clutter should be avoided because R is fundamentally based on object assignments. The less you assign as a variable in your environment the easier it will be to navigate complex scripts.\nThe good news is that you now know how to use the dplyr functions to wrangle data. The function names in dplyr were chosen specifically to be descriptive. This will make your code much more readable than if you were using base R counterparts. The bad news is that I haven’t told you how to easily link the functions. Fortunately, there’s an easy fix to this problem.\nThe magrittr package (comes with tidyverse) provides a very useful method called piping that will make wrangling a whole lot easier. The idea is simple: a pipe (%&gt;%) is used to chain functions together. The output from one function becomes the input to the next function in the pipe. This avoids the need to create intermediate objects and creates a logical progression of steps that demystify the wrangling process.\nConsider the simple example:\n\n# not using pipes, select a column, filter rows\nbad_ex &lt;- select(fishdat, Gear, `Common Snook`)\nbad_ex2 &lt;- filter(bad_ex, `Common Snook` &gt; 25)\n\nWith pipes, it looks like this:\n\n# with pipes, select a column, filter rows\ngood_ex &lt;- fishdat %&gt;% \n  select(Gear, `Common Snook`) %&gt;%\n  filter(`Common Snook` &gt; 25)\n\nNow we’ve created only one new object in our environment and we can clearly see that we select, then filter. The only real coding difference is now the select and filter functions only include the relevant information. You do not need to specify a data object as input to a function if you’re using piping. The pipe will always use the input that comes from above.\nA couple comments about piping:\n\nI find it very annoying to type the pipe operator (six key strokes!). RStudio has a nice keyboard shortcut: Crtl + Shift + M for Windows (use Cmd + Shift + M on a mac).\nIt’s convention to start a new function on the next line after a pipe operator. This makes the code easier to read and you can also comment out a single step in a long pipe.\nDon’t make your pipes too long, limit them to a particular data object or task.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#exercise-5",
    "href": "data_wrangling_1.html#exercise-5",
    "title": "2  Data wrangling part 1",
    "section": "2.8 Exercise 5",
    "text": "2.8 Exercise 5\nNow that we know how to pipe functions, let’s repeat exercise 4. You should already have code to select and filter the data. Use the following to repeat the analysis but with pipes. You should only have to create one data object in this exercise.\n\nUsing your code from exercise four, try to replicate the steps using pipes. The steps we used in exercise four were:\n\nFrom fishdat, select the Reference, Sampling_Date, Gear, and Sand Seatrout columns\nFilter by Gear to get only gear 20\nRename Sampling_Date to date\n\n\n\nClick to show/hide solution\nex2 &lt;- fishdat %&gt;% \n  select(Reference, Sampling_Date, Gear, `Sand Seatrout`) %&gt;% \n  filter(Gear == 20) %&gt;% \n  rename(date = Sampling_Date)\nhead(ex2)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#next-time",
    "href": "data_wrangling_1.html#next-time",
    "title": "2  Data wrangling part 1",
    "section": "2.9 Next time",
    "text": "2.9 Next time\nNow you should be able to answer (or be able to find answers to) the following:\n\nWhy do we need to manipulate data?\nWhat is the tidyverse?\nWhat can you do with dplyr?\nWhat is piping?\n\nNext we’ll continue with data wrangling.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_1.html#attribution",
    "href": "data_wrangling_1.html#attribution",
    "title": "2  Data wrangling part 1",
    "section": "2.10 Attribution",
    "text": "2.10 Attribution\nContent in this lesson was pillaged extensively from the USGS-R training curriculum here and R for data Science.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data wrangling part 1</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html",
    "href": "data_wrangling_2.html",
    "title": "3  Data wrangling part 2",
    "section": "",
    "text": "3.1 Lesson Outline\nGet the lesson R script: Data_Wrangling_2.R\nGet the lesson data: download zip",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#lesson-outline",
    "href": "data_wrangling_2.html#lesson-outline",
    "title": "3  Data wrangling part 2",
    "section": "",
    "text": "Combining data\nTidy data\nGroup by and summarize",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#lesson-exercises",
    "href": "data_wrangling_2.html#lesson-exercises",
    "title": "3  Data wrangling part 2",
    "section": "3.2 Lesson Exercises",
    "text": "3.2 Lesson Exercises\n\nExercise 6\nExercise 7\nExercise 8",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#goals",
    "href": "data_wrangling_2.html#goals",
    "title": "3  Data wrangling part 2",
    "section": "3.3 Goals",
    "text": "3.3 Goals\nIn this lesson we’ll continue our discussion of data wrangling with the tidyverse. Data wrangling is the manipulation or combination of datasets for the purpose of understanding. It fits within the broader scheme of data exploration, described as the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again (from R for Data Science, as is most of today’s content).\n\nAlways remember that wrangling is based on a purpose. The process always begins by answering the following two questions:\n\nWhat do my input data look like?\nWhat should my input data look like given what I want to do?\n\nYou define what steps to take to get your data from input to where you want to go.\nLast lesson we learned the following functions from the dplyr package (cheatsheet here):\n\nSelecting variables with select\nFiltering observations by some criteria with filter\nAdding or modifying existing variables with mutate\nRenaming variables with rename\nArranging rows by a variable with arrange\n\nAs before, we only have one hour to cover the basics of data wrangling. It’s an unrealistic expectation that you will be a ninja wrangler after this training. As such, the goals are to expose you to fundamentals and to develop an appreciation of what’s possible. I also want to provide resources that you can use for follow-up learning on your own.\nAfter this lesson you should be able to answer (or be able to find answers to) the following:\n\nHow are data joined?\nWhat is tidy data?\nHow do I summarize a dataset?\n\nYou should already have the tidyverse package installed, but let’s give it a go if you haven’t done this part yet:\n\n# install\ninstall.packages('tidyverse')\n\nAfter installation, we can load the package:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#combining-data",
    "href": "data_wrangling_2.html#combining-data",
    "title": "3  Data wrangling part 2",
    "section": "3.4 Combining data",
    "text": "3.4 Combining data\nCombining data is a common task of data wrangling. Perhaps we want to combine information between two datasets that share a common identifier. As a real world example, our fisheries data contain information about fish catch, but we also want to include spatial information about the stations (i.e., lat, lon). We would need to (and we will) combine data if this information is in two different places. Combining data with dplyr is called joining.\nAll joins require that each of the tables can be linked by shared identifiers. These are called ‘keys’ and are usually represented as a separate column that acts as a unique variable for the observations. The “Station ID” is a common key, but remember that a key might need to be unique for each row. It doesn’t make sense to join two tables by station ID if multiple site visits were made. In that case, your key should include some information about the site visit and station ID.\n\n3.4.1 Types of joins\nThe challenge with joins is that the two datasets may not represent the same observations for a given key. For example, you might have one table with all observations for every key, another with only some observations, or two tables with only a few shared keys. What you get back from a join will depend on what’s shared between tables, in addition to the type of join you use.\nWe can demonstrate types of joins with simple graphics. The first is an inner-join.\n\nThe second is an outer-join, and comes in three flavors: left, right, and full.\n\nIf all keys are shared between two data objects, then left, right, and full joins will give you the same result. I typically only use left_join just because it’s intuitive to me. This assumes that there is never any more information in the second table - it has the same or less keys as the original table.\nThe data we downloaded for this workshop included fisheries catch data and the locations of each station. If we want to plot any of the catch data by location, we need to join the two datasets.\n\n# load the fish data\nfishdat &lt;- read_csv('data/fishdat.csv')\n\n# load the station data\nstatloc &lt;- read_csv('data/statloc.csv')\n\n# join the two \njoindat &lt;- left_join(fishdat, statloc, by = 'Reference')\nhead(joindat)\n\n# A tibble: 6 × 14\n  OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Bluefish\n     &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;                 &lt;dbl&gt;\n1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38        0\n2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23        0\n4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23        0\n5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23        0\n6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23        0\n# ℹ 7 more variables: `Common Snook` &lt;dbl&gt;, Mullets &lt;dbl&gt;, Pinfish &lt;dbl&gt;,\n#   `Red Drum` &lt;dbl&gt;, `Sand Seatrout` &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#exercise-6",
    "href": "data_wrangling_2.html#exercise-6",
    "title": "3  Data wrangling part 2",
    "section": "3.5 Exercise 6",
    "text": "3.5 Exercise 6\nFor this exercise we’ll repeat the join we just did, but on a subset of the data. We’re going to select some columns of interest from our fisheries dataset, filter by gear type, then use a full_join with the station data. Try to use pipes if you can.\n\nSelect the Reference, Sampling_Date, Gear, and Common Snook columns from the fisheries dataset (hint, use select from dplyr).\nFilter the fisheries dataset by gear type 20 (hint, use filter == 20). Check the dimensions of the new dataset with dim.\nUse a full_join to join the fisheries dataset with the station location dataset. What is the key value for joining?\nCheck the dimensions of the new table. What happened? Select only the StationID, StationWaterDepth, SampleLatitude, and SampleLongitude columns from the master data.\n\n\n\nClick to show/hide solution\n# load the fish data\nfishdat &lt;- read_csv('data/fishdat.csv')\n\n# load the station data\nstatloc &lt;- read_csv('data/statloc.csv')\n\n# wrangle before join\njoindat &lt;- fishdat %&gt;% \n  select(Reference, Sampling_Date, Gear, `Common Snook`) %&gt;% \n  filter(Gear == 20) \n\ndim(joindat)\n\n# full join\njoindat &lt;- joindat %&gt;% \n  full_join(statloc, by = 'Reference')\n\ndim(joindat)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#tidy-data",
    "href": "data_wrangling_2.html#tidy-data",
    "title": "3  Data wrangling part 2",
    "section": "3.6 Tidy data",
    "text": "3.6 Tidy data\nThe opposite of a tidy dataset is a messy dataset. You should always strive to create a tidy data set as an outcome of the wrangling process. Tidy data are easy to work with and will make downstream analysis much simpler. This will become apparent when we start summarizing and plotting our data.\nTo help understand tidy data, it’s useful to look at alternative ways of representing data. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables country, year, population, and cases, but each dataset organises the values in a different way. Only one of these examples is tidy.\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n# Spread across two tibbles\ntable4a  # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b  # population\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nThese are all representations of the same underlying data but they are not equally easy to work with. The tidy dataset is much easier to work with inside the tidyverse.\nThere are three inter-correlated rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\nFor the example tables above, only the first table is tidy.\nThere are some very real reasons why you would encounter untidy data:\n\nMost people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data.\nData is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible.\n\nThis means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. The second step is to resolve one of two common problems:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\nTo fix these problems, you’ll need the two most important functions in tidyr: gather() and spread().\nSide note: Recent versions of tidyr have the pivot_longer() and pivot_wider() functions that accomplish similar tasks. Some may find these functions more intuitive, but I present the former here for legacy purposes.\n\n3.6.1 Gathering\nA common problem is a dataset where some of the column names are not names of variables, but values of a variable. Take table4a: the column names 1999 and 2000 represent values of the year variable, and each row represents two observations, not one.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\nTo tidy a dataset like this, we need to gather those columns into a new pair of variables. To describe that operation we need three parameters:\n\nThe set of columns that represent values, not variables. In this example, those are the columns 1999 and 2000.\nThe name of the variable whose values form the column names. I call that the key, and here it is year.\nThe name of the variable whose values are spread over the cells. I call that value, and here it’s the number of cases.\n\nTogether those parameters generate the call to gather():\n\ntable4a %&gt;%\n  gather(`1999`, `2000`, key = \"year\", value = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Brazil      1999   37737\n3 China       1999  212258\n4 Afghanistan 2000    2666\n5 Brazil      2000   80488\n6 China       2000  213766\n\n\nGathering can be graphically demonstrated:\n\n\n\n3.6.2 Spreading\nSpreading is the opposite of gathering. You use it when an observation is scattered across multiple rows. For example, take table2: an observation is a country in a year, but each observation is spread across two rows.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nTo tidy this up, we first analyse the representation in similar way to gather(). This time, however, we only need two parameters:\n\nThe column that contains variable names, the key column. Here, it’s type.\nThe column that contains values forms multiple variables, the value column. Here it’s count.\n\nOnce we’ve figured that out, we can use spread(), as shown programmatically below.\n\nspread(table2, key = type, value = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nSpreading can be graphically demonstrated:",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#exercise-7",
    "href": "data_wrangling_2.html#exercise-7",
    "title": "3  Data wrangling part 2",
    "section": "3.7 Exercise 7",
    "text": "3.7 Exercise 7\nLet’s take a look at the fisheries data. Are these data “tidy”? To help with the next few examples, we’ll gather the species and catch count data using the gather function from the tidyr package.\n\nInspect the fisheries dataset. What are the dimensions (hint: dim)? What are the names and column types (hint: str)?\nUse the gather function to “gather” the species columns and count data into two new columns. What are your keys? What are your values? Assign the new dataset to a variable in your environment.\nCheck the dimensions and structure of your new dataset. What’s different?\n\n\n\nClick to show/hide solution\n# check dimensions, structure\ndim(fishdat)\nstr(fishdat)\n\n# gather the fishdat\ngatherdat &lt;- fishdat %&gt;%\n  gather(key = 'Species', value = 'Count', Bluefish, `Common Snook`, Mullets, Pinfish, `Red Drum`, `Sand Seatrout`)\n\n# check dimensions, structure\ndim(gatherdat)\nstr(gatherdat)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#group-by-and-summarize",
    "href": "data_wrangling_2.html#group-by-and-summarize",
    "title": "3  Data wrangling part 2",
    "section": "3.8 Group by and summarize",
    "text": "3.8 Group by and summarize\nThe last tool we’re going to learn about in dplyr is the summarize function. As the name implies, this function lets you summarize columns in a dataset. Think of it as a way to condense rows using a summary method of your choice, e.g., what’s the average of the values in a column?\nThe summarize function is most useful with the group_by function. This function lets you define a column that serves as a grouping variable for developing separate summaries, as compared to summarizing the entire dataset. The group_by function works with any dplyr function so it can be quite powerful.\nLet’s use our gathered fisheries dataset from exercise 7.\n\nhead(gatherdat)\n\n# A tibble: 6 × 8\n  OBJECTID Reference Sampling_Date    yr  Gear ExDate              Species Count\n     &lt;dbl&gt; &lt;chr&gt;     &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   &lt;dbl&gt;\n1  1550020 TBM19960… 1996-03-20     1996   300 2018-04-12 10:27:38 Bluefi…     0\n2  1550749 TBM19960… 1996-03-20     1996    22 2018-04-12 10:25:23 Bluefi…     0\n3  1550750 TBM19960… 1996-03-20     1996    22 2018-04-12 10:25:23 Bluefi…     0\n4  1550762 TBM19960… 1996-03-22     1996    20 2018-04-12 10:25:23 Bluefi…     0\n5  1550828 TBM19960… 1996-04-26     1996   160 2018-04-12 10:25:23 Bluefi…     0\n6  1550838 TBM19960… 1996-05-13     1996   300 2018-04-12 10:25:23 Bluefi…     0\n\n\nIt’s difficult to see patterns until we start to evaluate some of the differences. It’s also setup in a way to let us easily group by different variables. We could ask a simple question: how does total catch vary across species (ignoring gear differences)?\nFirst we can use group_by. Notice the new information at the top of the output.\n\nby_spp &lt;- group_by(gatherdat, Species)\nby_spp\n\n# A tibble: 17,064 × 8\n# Groups:   Species [6]\n   OBJECTID Reference     Sampling_Date    yr  Gear ExDate              Species \n      &lt;dbl&gt; &lt;chr&gt;         &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;   \n 1  1550020 TBM1996032006 1996-03-20     1996   300 2018-04-12 10:27:38 Bluefish\n 2  1550749 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23 Bluefish\n 3  1550750 TBM1996032004 1996-03-20     1996    22 2018-04-12 10:25:23 Bluefish\n 4  1550762 TBM1996032207 1996-03-22     1996    20 2018-04-12 10:25:23 Bluefish\n 5  1550828 TBM1996042601 1996-04-26     1996   160 2018-04-12 10:25:23 Bluefish\n 6  1550838 TBM1996051312 1996-05-13     1996   300 2018-04-12 10:25:23 Bluefish\n 7  1550842 TBM1996051407 1996-05-14     1996   300 2018-04-12 10:25:23 Bluefish\n 8  1551131 TBM1996051415 1996-05-14     1996   300 2018-04-12 10:27:38 Bluefish\n 9  1551311 TBM1996032209 1996-03-22     1996   300 2018-04-12 10:25:20 Bluefish\n10  1551335 TBM1996041807 1996-04-18     1996    22 2018-04-12 10:25:24 Bluefish\n# ℹ 17,054 more rows\n# ℹ 1 more variable: Count &lt;dbl&gt;\n\n\nWe can then summarize to get the total count.\n\nby_spp &lt;- summarize(by_spp, totals = sum(Count))\nby_spp\n\n# A tibble: 6 × 2\n  Species       totals\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Bluefish           7\n2 Common Snook    3898\n3 Mullets           17\n4 Pinfish        98182\n5 Red Drum        3422\n6 Sand Seatrout   7789\n\n\nOf course, this can (and should) be done with pipes:\n\nby_spp &lt;- gatherdat %&gt;%\n  group_by(Species) %&gt;%\n  summarize(totals = sum(Count))\nby_spp\n\n# A tibble: 6 × 2\n  Species       totals\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Bluefish           7\n2 Common Snook    3898\n3 Mullets           17\n4 Pinfish        98182\n5 Red Drum        3422\n6 Sand Seatrout   7789\n\n\nWe can group the dataset by more than one column to get summaries with multiple groups. Here we can look at total count by each unique combination of gear and species.\n\nby_spp_gear &lt;- gatherdat %&gt;%\n  group_by(Species, Gear) %&gt;%\n  summarize(totals = sum(Count))\nby_spp_gear\n\n# A tibble: 36 × 3\n# Groups:   Species [6]\n   Species       Gear totals\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 Bluefish        20      1\n 2 Bluefish        22      0\n 3 Bluefish       160      6\n 4 Bluefish       251      0\n 5 Bluefish       300      0\n 6 Bluefish       301      0\n 7 Common Snook    20     20\n 8 Common Snook    22      0\n 9 Common Snook   160   3866\n10 Common Snook   251     12\n# ℹ 26 more rows\n\n\nWe can also get more than one summary at a time. The summary function can use any function that operates on a vector. Some common examples include min, max, sd, var, median, mean, and n. It’s usually good practice to include a summary of how many observations were in each group, so get used to including the n function.\n\nmore_sums &lt;-gatherdat %&gt;%\n  group_by(Species) %&gt;%\n  summarize(\n    n = n(),\n    min_count = min(Count),\n    max_count = max(Count),\n    total = sum(Count)\n  )\nmore_sums\n\n# A tibble: 6 × 5\n  Species           n min_count max_count total\n  &lt;chr&gt;         &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Bluefish       2844         0         2     7\n2 Common Snook   2844         0       127  3898\n3 Mullets        2844         0         5    17\n4 Pinfish        2844         0      2530 98182\n5 Red Drum       2844         0       123  3422\n6 Sand Seatrout  2844         0       560  7789\n\n\nFinally, many of the summary functions we’ve used (e.g., sum, min, max, mean) will not work correctly if there are missing observations in your data. You’ll see these as NA (or sometimes NaN) entries. You have to use the argument na.rm = T to explicitly tell R how to handle the missing values. Setting na.rm = T says to remove the NA values when summarizing the data.\n\nx &lt;- c(1, 2, NA, 4)\nmean(x)\n\n[1] NA\n\nmean(x, na.rm = T)\n\n[1] 2.333333\n\n\nA quick check for missing values can be done with anyNA. This works on vectors and data frames.\n\nanyNA(x)\n\n[1] TRUE",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#exercise-8",
    "href": "data_wrangling_2.html#exercise-8",
    "title": "3  Data wrangling part 2",
    "section": "3.9 Exercise 8",
    "text": "3.9 Exercise 8\nNow we have access to a several tools in the tidyverse to help us wrangle more effectively. For the final exercise, we’re going subset our fisheries data and get a summary. Specifically, we’ll filter our data by Gear type 20 and Pinfish, then summarize the average catch across stations.\n\nUsing the gathered data we created in exercise 7, filter the data by Gear type 20 and Pinfish (hint, filter(Gear == 20 & Species == 'Pinfish').\nGroup the data by site so you can summarize the catch of Pinfish (hint, group_by(Reference).\nSummarize the count column by taking the average (hint, summarise(ave = mean(Count)))\nWhich station has the highest average catch of Pinfish (hint, arrange(ave))?\n\n\n\nClick to show/hide solution\nsumdat &lt;- gatherdat %&gt;%\n  filter(Gear == 20 & Species == 'Pinfish') %&gt;% \n  group_by(Reference) %&gt;% \n  summarize(\n    ave = mean(Count)\n  ) %&gt;% \n  arrange(-ave)\nsumdat",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#next-time",
    "href": "data_wrangling_2.html#next-time",
    "title": "3  Data wrangling part 2",
    "section": "3.10 Next time",
    "text": "3.10 Next time\nNow you should be able to answer (or be able to find answers to) the following:\n\nHow are data joined?\nWhat is tidy data?\nHow do I summarize a dataset?\n\nIn the next lesson we’ll learn about data visualization and graphics.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_wrangling_2.html#attribution",
    "href": "data_wrangling_2.html#attribution",
    "title": "3  Data wrangling part 2",
    "section": "3.11 Attribution",
    "text": "3.11 Attribution\nContent in this lesson was pillaged extensively from the USGS-R training curriculum here and R for data Science.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data wrangling part 2</span>"
    ]
  },
  {
    "objectID": "data_viz.html",
    "href": "data_viz.html",
    "title": "4  Data viz",
    "section": "",
    "text": "4.1 Lesson Outline\nGet the lesson R script: Viz_and_Graphics.R\nGet the lesson data: download zip",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#lesson-outline",
    "href": "data_viz.html#lesson-outline",
    "title": "4  Data viz",
    "section": "",
    "text": "Base R graphics\nGGplot2\n\nBasics\nModifying plot components\n\nSaving your plots",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#lesson-exercises",
    "href": "data_viz.html#lesson-exercises",
    "title": "4  Data viz",
    "section": "4.2 Lesson Exercises",
    "text": "4.2 Lesson Exercises\n\nExercise 9\nExercise 10\nExercise 11",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#goals",
    "href": "data_viz.html#goals",
    "title": "4  Data viz",
    "section": "4.3 Goals",
    "text": "4.3 Goals\nThis section will introduce you to concepts of data visualization and graphics in R. The entire workflow of data exploration is enhanced through looking at your data, whether you’re exploring a dataset for the first time or creating publication-ready figures. Viewing your data provides insight into patterns that can help you explore different hypotheses. No analysis is complete without a solid graphic.\n\nGraphics capabilities in R have improved tremendously in the last ten years. The de facto plotting library is ggplot2 as part of the tidyverse, but there are many other packages available that enhance or build on existing functionality. The base R installation also comes with its own set of plotting functions. These are useful for quick and dirty exploration but you’ll quickly find that these methods are tedious. We’ll start this lesson with a cursory look at some of the base R plotting functions. Later, we’ll focus on ggplot2.\nAfter this lesson you should be able to answer (or be able to find answers to) the following:\n\nWhat can I do with base R plots?\nWhat are the requirements of every ggplot?\nWhat are geoms?\nWhat are facets?\nWhat are themes?\nHow do I save a plot?",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#base-r-graphics",
    "href": "data_viz.html#base-r-graphics",
    "title": "4  Data viz",
    "section": "4.4 Base R graphics",
    "text": "4.4 Base R graphics\nIn theory, all of your plotting needs can be accomplished with functions in base R. However, these graphics are not easy to customize and it’s very tedious to create publication-quality graphics. For these reasons, base R graphics are rarely taught but I believe they still serve a purpose in the data exploration workflow. Base R graphics are extremely fast and simple to use for exploring data. I caution you in using these functions for anything beyond simple exploration.\nMy most frequent use for base R graphics include:\n\nSimple scatterplots to see how two variables are related (plot)\nHistograms to explore the distribution of a variable (hist)\n\nWe’ll explore some of these simple tools in base R with the mpg dataset that comes with R. The workhorse plotting function is plot. Let’s make a simple plot of highway mileage vs engine size.\n\nlibrary(tidyverse) # for them mpg dataset\ndata(mpg)\nplot(hwy ~ displ, data = mpg)\n\n\n\n\n\n\n\n\nWe created this plot using the formula notation. We can also create the plot by calling the columns directly.\n\nplot(mpg$displ, mpg$hwy)\n\n\n\n\n\n\n\n\nConfusing right? Let’s have a look at the help file for plot.\n\n?plot\n\n\nThis help file lays out some of the basics of plot formatting. We can change the type of plot using the type argument (here, we can only use points), give it a title with main, and change labels on the x and y axes with xlab and ylab.\n\nplot(hwy ~ displ, data = mpg, main = 'Highway mileage by engine size', \n     xlab = 'engine size (l)', ylab = 'highway (mpg)')\n\n\n\n\n\n\n\n\nThe help file for the plot function also has this ambiguous ... argument. You’ll see this argument for many functions in the help files. This means that the function accepts arguments from other functions. In this case, the plot function accepts arguments from the par function which is used for setting graphical parameters in base R. Let’s look at the help file for par.\n\n?par\n\n\nThe most useful section in this help file is the description of the actual graphical parameters you can set. Some useful ones are the cex family of arguments for sizing, col family of arguments for color, the pch argument for point types, and the family argument for font. All of these arguments can be passed directly to the plot function or separately with the par function.\n\nplot(hwy ~ displ, data = mpg, main = 'Highway mileage by engine size', \n     xlab = 'engine size (l)', ylab = 'highway (mpg)', \n     col = 'blue', pch = 18, cex = 0.75, family = 'serif')\n\n\n\n\n\n\n\n\nOr like this…\n\npar(col = 'blue', pch = 18, cex = 0.75, family = 'serif')\nplot(hwy ~ displ, data = mpg, main = 'Highway mileage by engine size', \n     xlab = 'engine size (l)', ylab = 'highway (mpg)')\n\n\n\n\n\n\n\n\nNotice how the plots are different depending on where the additional arguments are used. Confusing, right? You’ll waste a lot of time trying to tweak the base graphics.\nThe hist (histogram) function is the only other plotting function from base R that is worth showing. This gets you info on the basic distribution of a variable and is a graphical way of assessing the spread and central tendency (i.e., variance and mean) of a variable.\n\nhist(mpg$hwy)\n\n\n\n\n\n\n\n\nThe only useful argument in hist is breaks that controls the number of bins.\n\nhist(mpg$hwy, breaks = 20)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#exercise-9",
    "href": "data_viz.html#exercise-9",
    "title": "4  Data viz",
    "section": "4.5 Exercise 9",
    "text": "4.5 Exercise 9\nLet’s make a simple graphic of our fisheries dataset using base R. We’ll make a time series plot of catch using the plot function. We’ll also add a title and better axis labels to the plot to make it slightly more palatable.\n\nFirst, start by loading the fish data as you’ve done before. Use read_csv() from the tidyverse.\nMake a time series plot of Pinfish catch over time using the plot function. Try creating the plot using the formula notation (hint: Pinfish ~ Sampling_Date). Remember you have to use the data argument when you’re using the formula (i.e., data = fishdat).\nGive the plot an informative title using the main argument directly in the plot function, something like main = \"Pinfish catch for all stations over time\".\n\n\n\nClick to show/hide solution\n# load tidyverse\nlibrary(tidyverse)\n\n# load the fish data\nfishdat &lt;- read_csv('data/fishdat.csv')\n     \n# time series plot\nplot(Pinfish ~ Sampling_Date, data = fishdat)\n\n# time series plot with title\nplot(Pinfish ~ Sampling_Date, data = fishdat, main = 'Pinfish catch for all stations over time')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#ggplot2",
    "href": "data_viz.html#ggplot2",
    "title": "4  Data viz",
    "section": "4.6 GGplot2",
    "text": "4.6 GGplot2\n\n4.6.1 Basics\nThe ggplot2 package (full reference) is a huge improvement over base R because it was developed following a strict philosophy known as the grammar of graphics. This philosophy was designed to make thinking, reasoning, and communicating about graphs easier by following a few simple rules. Like building a sentence in speech (aka grammar), all graphs start with a foundational component that is used for building other graph pieces.\nWith ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) creates an empty base graph.\n\nggplot(data = mpg)\n\nThe next step is to add one or more layers (aka geoms) to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. Ggplot2 comes with many geom functions that each add a different type of layer to a plot.\n\nggplot(data = mpg) +\n  geom_point()\n\nEach geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is defined with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes. ggplot2 looks for the mapped variable in the data argument, in this case, mpg.\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy))\n\nJust remember these requirements:\n\nAll ggplot plots start with the ggplot function\nIt will need three pieces of information: the data, how the data are mapped to the plot aesthetics, and a geom layer\n\nThe core unit of every ggplot looks like this:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nApplied to the data:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\nMore commonly, the aes function that defines the mapping is included in the initial call to ggplot. This will globally define the mapping to all geoms in a plot, instead of for only one geom. There may be different reasons to globally apply the aesthetics or separately for each geom depending on the data.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point()\n\n\n\n\n\n\n\n\nWhy the need for this complicated structure? The syntax of mapping aesthetics to a dataset lets you easily modify components of an existing plot. Additional datasets and geoms can easily be added to the plot with +.\nLet’s explore some of the other geoms.\nAs lines…\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_line()\n\n\n\n\n\n\n\n\nAs counts…\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_count()\n\n\n\n\n\n\n\n\nAs density…\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_density2d()\n\n\n\n\n\n\n\n\nAs a line range…\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_linerange()\n\nError: geom_linerange requires the following missing aesthetics: ymin, ymax\nExecution halted\nOh snap, what happened? This error is telling us that the geom we just tried is missing some required aesthetics in the plot. Here we’ve only used the x and y aesthetics but it looks like it requires ymin and ymax. Lets look at the help file for geom_linerange.\n\n?geom_linerange\n\n\nLooks like we don’t have the required aeshetics, nor does it make sense to use this geom because it’s not appropriate for the data. What about the requirements for geom_point?\n\n?geom_point\n\n\nWe’ve got the required aesthetics in our plot. Let’s add some others that we can use with geom_point.\nChanging the color:\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point()\n\n\n\n\n\n\n\n\nChanging the size:\n\nggplot(mpg, aes(x = displ, y = hwy, size = displ)) + \n  geom_point()\n\n\n\n\n\n\n\n\nChanging the shapes:\n\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_point()\n\n\n\n\n\n\n\n\nIn all of the above examples we’ve mapped an aesthetic to a variable in our dataset. We can just as easily modify the plot without mapping it to a variable (i.e., changing a part of the plot independent of the data). For example, maybe we want to change the color of the points using a single color for everything. Notice the placement of colour outside of the aes mapping function.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(colour = 'red')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#exercise-10",
    "href": "data_viz.html#exercise-10",
    "title": "4  Data viz",
    "section": "4.7 Exercise 10",
    "text": "4.7 Exercise 10\nLet’s reproduce the time series plot we created in exercise 9 using ggplot. This requires us to map the Sampling_Date and Pinfish variables to the x and y aesthetics for geom_point.\n\nSetup your initial plot with ggplot. Map the x variable to Sampling_Date, the y variable to Pinfish. The setup should look like this: ggplot(fishdat, aes(x = Sampling_Date, y = Pinfish))). What happens if you run only this code?\nAdd the geom_point() geom to your plot using the + operator. Remember the correct placement of +, it always occurs in the line preceding the layer that is being added to the plot.\nThe catch data are heavily skewed. We can easily transform the y-axis by adding a scale. Ggplot has multiple scale functions that accomplish different tasks, all of which relate to setting limits or characteristics of measured variables in your data. Add the following after geom_point() (remember to put a + after geom_point()): scale_y_continuous('log-Catch, Pinfish', trans = 'log10'). How does the plot look now?\n\n\n\nClick to show/hide solution\nggplot(fishdat, aes(x = Sampling_Date, y = Pinfish)) +\n  geom_point() + \n  scale_y_continuous('log-Catch, Pinfish', trans = 'log10')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#modifying-plot-components",
    "href": "data_viz.html#modifying-plot-components",
    "title": "4  Data viz",
    "section": "4.8 Modifying plot components",
    "text": "4.8 Modifying plot components\nThere are countless ways we can modify a ggplot, either by manipulating the appearance or adding information that improves our understanding of what we see. In the previous exercise, we used a scale to transform an axis. In this next section we’ll cover three additional ways to modify a plot:\n\nModifying the appearance of the plot as a whole can be done using the theme function for individual parts or by using a pre-packaged theme that modifies many parts at once.\nAdding statistical summaries with stat_smooth\nMaking multi-panel plots with facet_wrap or facet_grid.\n\nFirst I’ll show you how to modify the appearance to your liking. Individual components can be modified with theme. Here the legend is moved to the top, the minor grid lines between axis ticks are removed, and the gray panel background is changed to light blue. Check the help file for theme to see all the options.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  theme(\n    legend.position = 'top',\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = 'lightblue')\n  )\n\n\n\n\n\n\n\n\nChanging plot elements with theme can take some practice because there’s lots to modify. In some respects, this is how ggplot is similar to base R. With flexibility comes tedium. Fortunately, there are several pre-packaged themes that modify several components at once. See here for the full documentation. There are also additional packages available that supplement the existing themes inggplot (see here).\nBlack and white:\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  theme_bw()\n\n\n\n\n\n\n\n\nMinimal:\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nClassic:\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\n\n\n\n\nThe pre-packaged themes with ggplot have the added benefit of easily changing the global font types and sizes in the plot. These are modified by including the arguments base_family and base_size.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  theme_bw(base_family = 'serif', base_size = 16)\n\n\n\n\n\n\n\n\nIn addition to its functional syntax, the real power of ggplot is the ability to add components to the plot that let you quickly evaluate relationships or trends in the data. Statistical relationships can be added with stat_smooth.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  stat_smooth()\n\n\n\n\n\n\n\n\nNotice that we get one smooth for each type of drive train. This is because we’ve mapped colour to the drive train. Both the geom_smooth and stat_smooth functions use colour as an aesthetic so the global aes function applies to both. We can change this behavior by moving the location of the colour aesthetic. Here color is only mapped to the points.\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(colour = drv)) + \n  stat_smooth()\n\n\n\n\n\n\n\n\nBy default the stat_smooth function uses a non-linear smooth, either a locally estimated polynomial or generalized additive model depending on size of the dataset. We can change this using the method argument, as for a linear model.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  stat_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nFinally, ggplot provides some simple functions to create multi-panelled plots. This is useful for viewing different groups of the data as they relate to a variable in the dataset. The facet_wrap function is one of two functions in ggplot that can be used for multi-panel plots (the other being facet_grid, which is similar but different). To create facets, we have to specify which variable you’re using that acts as a grouping variable for each subplot. This is defined using the ~ sign followed by the variable name within facet_wrap. The ncol (or nrow) argument also indicates how many columns (or rows) in the multi-panel to create.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  stat_smooth(method = 'lm') + \n  facet_wrap(~ drv, ncol = 3)\n\n\n\n\n\n\n\n\nThe scales argument is also a useful part of facet_wrap. By default, the x and y axes are fixed between the panels. You can change this behavior by using scales = \"free\". Be careful using this feature because it can lead to different interpretations of the magnitude of trends.\n\nggplot(mpg, aes(x = displ, y = hwy, colour = drv)) + \n  geom_point() + \n  stat_smooth(method = 'lm') + \n  facet_wrap(~ drv, ncol = 3, scales = 'free')",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#exercise-11",
    "href": "data_viz.html#exercise-11",
    "title": "4  Data viz",
    "section": "4.9 Exercise 11",
    "text": "4.9 Exercise 11\nLet’s modify the time series plot we created in exercise 10 by adding a theme, a statistical summary, and facets. We’ll map the stat_smooth function to the Gear type and also use this variable to create a three-panel plot.\n\nSetup the initial plot as before. Map the x variable to Sampling_Date and the y variable to Pinfish. The intiial ggplot line will look like this ggplot(fishdata, aes(x = Sampling_Date, y = Pinfish). Add the geom_point geom and the scale_y_continuous scale with trans = \"log10\" to transform catch.\nAdd one of these thems to the plot: theme_bw(), theme_classic(), or theme_minimal().\nAdd the stat_smooth function with the argument method = \"lm\" to add linear smooths between catch and time for each gear type.\nUse facet_wrap to create a three-panel plot by depth category (hint: facet_wrap(~ Gear, ncol = 3)).\n\n\n\nClick to show/hide solution\nggplot(fishdat, aes(x = Sampling_Date, y = Pinfish)) +\n  geom_point() + \n  scale_y_continuous('log-Catch, Pinfish', trans = 'log10') + \n  theme_minimal() + \n  stat_smooth(method = 'lm') + \n  facet_wrap(~ Gear, ncol = 3)",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#saving-your-plots",
    "href": "data_viz.html#saving-your-plots",
    "title": "4  Data viz",
    "section": "4.10 Saving your plots",
    "text": "4.10 Saving your plots\nA quality plot deserves to be saved and shared. As you can imagine, there are several ways to save a plot in RStudio. The easiest way (which I don’t recommend) is to use the export feature from the plot viewing pane.\n\nAlthough this is convenient, you don’t have fine control over many options that can really make your graphic pop. I recommend using either the ggsave function that comes with the tidyverse, or preferably, one of the available graphics devices from base R (bmp, jpeg, png, tiff, eps, ps, tex, svg, wmf, and my favorite, pdf).\nThe ggsave function works only with ggplot objects. You can contol where the plot is saved, the file type, plotting dimensions, resolution, and a few other minor options. By default, it will save the last ggplot that you made.\n\nggsave('figure/myfig.jpg', device = 'jpeg', width = 5, height = 4, units = 'in', dpi = 300)\n\nThe ggsave functions uses the graphics devices from base R behind the scenes. You can always use these directly to save any type of plot (ggplot, base, etc.). These work a bit differently from regular functions. First, you “open” a graphics device by executing the function (e.g., png(), jpeg(), etc.) at the command line. Then you execute your plot command and finish by “closing” the device with dev.off(). In a script, it will look something like this.\n\n# save a plot as png file\npng('figure/myfig.png', width = 5, height = 4, units = 'in', res = 300)\nplot\ndev.off()\n\nThe value of this approach is the ability to fine tune the output for your graphics. You can save as many figures as you like when the graphics device is open, just make sure to use dev.off() when you’re done.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#next-time",
    "href": "data_viz.html#next-time",
    "title": "4  Data viz",
    "section": "4.11 Next time",
    "text": "4.11 Next time\nNow you should be able to answer (or be able to find answers to) the following:\n\nWhat can I do with base R plots?\nWhat are the requirements of every ggplot?\nWhat are geoms?\nWhat are facets?\nWhat are themes?\nHow do I save a plot?\n\nIn the next lesson we’ll learn about the basics of spatial analysis in R.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "data_viz.html#attribution",
    "href": "data_viz.html#attribution",
    "title": "4  Data viz",
    "section": "4.12 Attribution",
    "text": "4.12 Attribution\nContent in this lesson was pillaged extensively from the USGS-R training curriculum here and R for data Science.",
    "crumbs": [
      "Lessons",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data viz</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Appendix A — Setup for the workshop",
    "section": "",
    "text": "B Setup for the workshop\nThanks for your interest in the Introduction to R workshop. You will need to do the following, outlined below, before the workshop.\nMost of these steps will require administrative privileges on a computer. Work with your IT staff to complete the setup if you do not have these privileges. As an alternative, you can use Posit Cloud to participate (see Section B.2), although we strongly encourage you to install the software on your personal computer for use after the workshop.\nPlease reach out if you have any issues with installation: mbeck@tbep.org",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#install-r-and-rstudio",
    "href": "setup.html#install-r-and-rstudio",
    "title": "Appendix A — Setup for the workshop",
    "section": "B.1 Install R and RStudio",
    "text": "B.1 Install R and RStudio\nR and RStudio are separate downloads and installations. R is the underlying statistical computing software. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio.\nThanks to the USGS-R Training group and Data Carpentry for making their installation materials available. The following instructions come directly from their materials, with a few minor edits to help you get set up.\n\nB.1.1 Windows: Download and install R\nGo to CRAN and download the R installer for Windows. Make sure to choose the latest stable version (v4.5.1 as of October 2025).\nOnce the installer downloads, Right-click on it and select “Run as administrator”.\nType in your credentials and click yes (or if you don’t have administrator access have your IT rep install with Admin privileges).\n\nYou can click next through the standard dialogs and accept most defaults. But at the destination screen, please verify that it is installing it to C:\\Program Files\\R\n\nAt the “Select Components” screen, you can accept the default and install both 32-bit and 64-bit versions.\n\nAt this screen, uncheck ‘Create a desktop icon’ because non-admin users in Windows will be unable to delete it.\n\n\n\nB.1.2 Windows: Download and install RStudio\nDownload RStudio from here.\nAfter download, double-click the installer. It will ask for your administrator credentials to install (you might need to have your IT rep install again).\nAccept all the default options for the RStudio install.\n\n\n\nB.1.3 macOS: Download and install R\n\nDownload and install R from the CRAN website for Mac here.\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R\nIt is also a good idea to install XQuartz (needed by some packages)\n\n\n\nB.1.4 macOS: Download and install RStudio\n\nGo to the RStudio download page\nUnder Installers select the appropriate RStudio download file for macOS\nDouble click the file to install RStudio\n\n\n\nB.1.5 Check Install\nOnce installed, RStudio should be accessible from the start menu. Start up RStudio. Once running it should look something like this:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  },
  {
    "objectID": "setup.html#sec-cloud",
    "href": "setup.html#sec-cloud",
    "title": "Appendix A — Setup for the workshop",
    "section": "B.2 Posit Cloud (optional)",
    "text": "B.2 Posit Cloud (optional)\nPosit Cloud provides an environment to use RStudio and the resources above through a web browser. We’ve created a workspace on Posit Cloud that includes most of the content described above. Open the following URL in a web browser: https://posit.cloud/content/11213234\nYou will see a login screen that looks like this:\n\nSign up using a personal login or existing account (Google, GitHub, etc.).\nYou’ll see the workspace in your browser once you’ve signed in. You’ll need to make a permanent copy to save your work. Just click the button at the top marked “+ Save as Permanent Copy”. When this is done, the red text at the top indicating “TEMPORARY COPY” will no longer be visible.\n\nNow you can follow along with the workshop content.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Setup for the workshop</span>"
    ]
  }
]